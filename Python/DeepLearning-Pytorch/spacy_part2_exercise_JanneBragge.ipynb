{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SpaCy - part 2\n",
    "\n",
    "Let's see how we can convert text to numbers and what we can do with the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print your name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise by: Janne Bragge\n"
     ]
    }
   ],
   "source": [
    "## Your code here \n",
    "print(\"Exercise by: Janne Bragge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SpaCy and load pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Import SpaCy \n",
    "\n",
    "- import spacy\n",
    "- print spacy version\n",
    "- You may need to download SpaCy text processing pipelines `de_core_news_sm`, `en_core_web_lg` and `fi_core_news_lg`. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m spacy download fi_core_news_lg\n",
    "!pip install spacy==3.7.2\n",
    "!pip install spacy -U spacu\n",
    "!pip install sacrebleu\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.4\n"
     ]
    }
   ],
   "source": [
    "## Task 1:\n",
    "## Your code here \n",
    "import spacy\n",
    "print(spacy.__version__)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings to hashes\n",
    "\n",
    "- Look up the string “cat” in `nlp.vocab.strings` to get the hash.\n",
    "- Look up the hash to get back the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'de_core_news_sm' (3.7.0) was trained with spaCy v3.7.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "cat =  5439657043933447811\n",
      "Katze =  13148695887159701572\n",
      "German:\n",
      "cat =  5439657043933447811\n",
      "Katze =  13148695887159701572 \n",
      "\n",
      "English & cat hash: cat\n",
      "German & cat hash: cat\n",
      "German & Katze hash: Katze\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "print('English:')\n",
    "code_en1 = nlp_en.vocab.strings[\"cat\"]\n",
    "code_en2 = nlp_en.vocab.strings[\"Katze\"]\n",
    "print('cat = ', code_en1)\n",
    "print('Katze = ',code_en2)\n",
    "print('German:')\n",
    "code_de1 = nlp_de.vocab.strings[\"cat\"]\n",
    "code_de2 = nlp_de.vocab.strings[\"Katze\"]\n",
    "print('cat = ', code_de1)\n",
    "print('Katze = ',code_de2, '\\n')\n",
    "print('English & cat hash:', nlp_en.vocab.strings[code_en1])\n",
    "print('German & cat hash:', nlp_de.vocab.strings[code_en1])\n",
    "print('German & Katze hash:', nlp_de.vocab.strings[code_de2])\n",
    "\n",
    "import gc\n",
    "del nlp_en\n",
    "del nlp_de\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - PERSON vs. Person vs. person hash?\n",
    "- Use package `en_core_web_sm`\n",
    "- Create function `str_to_hash(string)` to get hash codes for strings “PERSON / Person / person”.\n",
    "- Create function `hash_to_str(hash)` to get strings for hash codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2:\n",
    "## Your code here \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def str_to_hash(string):\n",
    "    \"\"\"Get the hash code for a string.\"\"\"\n",
    "    return nlp.vocab.strings[string]\n",
    "\n",
    "def hash_to_str(hash):\n",
    "    \"\"\"Get the string representation for a hash code.\"\"\"\n",
    "    return nlp.vocab.strings[hash]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash codes:\n",
      "380\n",
      "2313063860588076218\n",
      "14800503047316267216\n",
      "\n",
      "Strings:\n",
      "PERSON\n",
      "Person\n",
      "person\n"
     ]
    }
   ],
   "source": [
    "print('Hash codes:')\n",
    "print(str_to_hash(\"PERSON\"))\n",
    "print(str_to_hash(\"Person\"))\n",
    "print(str_to_hash(\"person\"))\n",
    "\n",
    "print('\\nStrings:')\n",
    "print(hash_to_str(380))\n",
    "print(hash_to_str(2313063860588076218))\n",
    "print(hash_to_str(14800503047316267216))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Why does this code throw an error?\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "code = nlp_de.vocab.strings[\"Katze\"]\n",
    "word = nlp_en.vocab.strings[code]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here...*\n",
    "\n",
    "Koodi aiheuttaa virheen, koska siinä yritetään käyttää saksankielisen sanaston hash-koodia englanninkielisessä sanastossa. Sanastot ovat kielikohtaisia, eikä saksankielisen sanan hash-koodi vastaa mitään merkintää englanninkielisessä sanastossa. Tämä johtaa virheeseen, kun koodi yrittää hakea sanaa englanninkielisestä sanastosta käyttäen saksankielistä hash-koodia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - And why this works?\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "code = nlp_de.vocab.strings[\"Katze\"]\n",
    "\n",
    "nlp_en.vocab.strings.add(\"Katze\")\n",
    "word = nlp_en.vocab.strings[code]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here...*\n",
    "\n",
    "Koodi toimii, koska se lisää sanan \"Katze\" eksplisiittisesti englanninkieliseen sanastoon käyttäen nlp_en.vocab.strings.add(\"Katze\"). Tämä luo merkinnän \"Katze\" sanalle englanninkieliseen sanastoon ja antaa sille uuden hash-koodin. Nyt kun koodi yrittää hakea sanaa nlp_en.vocab.strings[code], se löytää vastaavan merkinnän englanninkielisestä sanastosta, vaikka alkuperäinen hash-koodi tulikin saksankielisestä sanastosta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting word vectors\n",
    "\n",
    "SpaCy uses word vectors and with word vectors you can do much more than with just hash codes. \n",
    "\n",
    "#### Exercise - \"bananas\" word vector 1\n",
    "- Use small `en_core_web_sm` model \n",
    "- Print the vector for `\"bananas\"` using the `token.vector` attribute.\n",
    "- How long word vector is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3:\n",
    "## Your code here \n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Get the vector for \"bananas\"\n",
    "doc = nlp(\"bananas\")\n",
    "bananas_vector_sm = doc[0].vector\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7758115   1.6605448  -0.39189708 -0.32252163  0.78028345 -0.4860569\n",
      "  1.3714446   1.0607959  -0.33311647 -0.31107557  0.76571095 -0.18480885\n",
      " -0.19373725 -0.10193576 -0.05747509  1.138708   -0.5497646   0.27067184\n",
      "  0.6271011  -1.2856631  -1.3586081  -0.19643408 -0.1460527   0.26088634\n",
      "  0.80363554  0.11018915  0.62885725 -0.8537091  -0.38757703  0.3829352\n",
      " -0.40778205  1.0657502  -0.69960725  0.61539143 -1.0997975  -0.7574061\n",
      " -0.38754362  1.1433274  -0.01413831  0.01014978  0.6591927  -0.7107739\n",
      " -0.9430653  -0.69120467  0.3341462   0.09536986 -0.2903002   0.11057171\n",
      " -0.53214103 -1.05284     0.49351197 -0.62800545 -0.6515595  -0.5959477\n",
      "  0.02242928  1.2707175   0.9217299   0.46632504  0.5323633   0.75816953\n",
      " -0.76025885  0.08518691  0.17911062 -0.8465437   0.02004845  1.1643531\n",
      "  0.7171085  -0.87006867  0.00917868 -1.0137509  -0.5400829   0.18582483\n",
      "  0.49860662  0.5396634   0.00840527 -0.7188309   0.69892013 -0.7034926\n",
      " -1.2734017  -0.08274153 -0.1123147  -0.3993013   0.08494474  0.12980603\n",
      "  0.9206839   1.4036019  -0.321168   -0.22778934 -0.04201303 -0.13672091\n",
      " -0.431881    0.55883276  1.2955695  -0.04025474 -0.50777465 -0.5220174 ]\n",
      "Vector length: 96\n"
     ]
    }
   ],
   "source": [
    "print(bananas_vector_sm)\n",
    "print(\"Vector length:\", len(bananas_vector_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - \"bananas\" word vector 2\n",
    "- Repeate same with large model `en_core_web_lg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_lg' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "## Task 4:\n",
    "## Your code here \n",
    "\n",
    "# Load the large English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Get the vector for \"bananas\"\n",
    "doc = nlp(\"bananas\")\n",
    "bananas_vector_lg = doc[0].vector\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.1689e-01 -2.5989e+00 -1.3144e+00  2.2500e+00 -4.6767e-01 -2.0695e+00\n",
      " -6.3379e-01 -4.0222e-01 -3.4022e+00 -3.6932e-01 -7.9938e-01 -1.0412e+00\n",
      "  9.3756e-01  1.6070e+00  8.8330e-01 -2.8483e+00  1.3349e-01 -3.1656e+00\n",
      "  8.1896e-01 -4.8113e+00  1.5655e+00  1.6665e+00 -4.7081e-01 -1.9475e+00\n",
      " -1.1779e+00 -1.3810e+00 -2.0071e+00 -2.1639e-01  9.0609e-01  1.5279e+00\n",
      "  1.2587e-04 -2.9000e+00  7.6069e-01 -2.2825e+00  1.2495e-02 -1.5653e+00\n",
      "  2.0052e+00 -1.7747e+00  5.9220e-01 -1.1428e+00 -1.3441e+00  3.4784e-01\n",
      "  1.7492e+00  1.9086e+00  1.0600e+00  1.2965e+00  4.1431e-01  7.9416e-01\n",
      " -1.1277e+00 -1.1403e+00  7.5891e-01 -9.4419e-01  1.4413e+00 -2.2554e+00\n",
      "  1.6226e-01  3.8901e-01  1.2299e-01  1.1577e+00  1.5524e+00  1.3853e+00\n",
      "  1.1112e+00  7.5767e-01  3.9431e+00 -2.8506e-01 -2.1645e+00 -1.0862e+00\n",
      " -1.4973e+00 -1.2781e+00  2.4643e+00 -1.5886e+00  2.5679e-01  6.4918e-01\n",
      "  1.6809e-01  5.7693e-01  3.1121e-01 -4.5278e-01 -2.7555e+00 -2.1846e+00\n",
      "  4.4865e+00  2.7107e-01 -5.3831e-01  8.3013e-01  6.7752e-02  1.4234e-01\n",
      "  1.2585e+00 -8.5423e-01  9.2971e-01 -3.9940e-01 -5.8663e-01  6.6604e-01\n",
      "  2.3871e+00  4.9333e-01  2.3922e+00 -3.7396e+00 -3.9524e-01 -6.3799e-01\n",
      "  3.3500e+00 -2.0430e+00 -1.5601e+00 -2.3594e+00 -1.4671e+00 -3.2848e+00\n",
      "  1.5197e+00 -1.1674e+00 -1.2885e+00  3.4890e+00 -4.0526e+00  1.6946e+00\n",
      " -1.5310e+00  2.6790e+00  1.2865e+00 -2.9286e-02  3.1037e+00  1.3635e-01\n",
      " -1.3327e-01  5.4603e-01  1.2937e+00 -2.3662e+00  2.8862e-01  1.6226e+00\n",
      "  6.3531e-01  1.5498e+00 -2.3349e+00  1.6150e+00 -2.0071e+00 -4.8784e-01\n",
      "  1.7768e+00 -2.6920e+00  1.6341e+00  4.0537e-01  3.7324e-01  7.8494e-01\n",
      "  1.3917e+00 -2.9035e-01  2.5224e+00  1.2485e+00 -1.5159e+00 -2.0832e+00\n",
      " -1.8766e-01 -1.3394e+00 -5.3597e-01 -2.4915e+00  1.6341e+00 -3.0336e+00\n",
      "  1.8791e-01 -2.4776e-01  1.6347e+00 -7.0009e-01  2.1221e+00 -2.3470e+00\n",
      " -2.1513e+00  1.2630e+00 -2.8195e-01 -5.6535e-01 -9.0373e-01  1.3455e+00\n",
      "  4.1099e+00  6.9325e-01  4.6835e-01  2.9949e-01 -3.2456e-01 -2.1713e+00\n",
      "  4.6691e-01 -1.3795e-01  6.0910e-01 -1.3374e+00 -8.3586e-01  1.8260e+00\n",
      " -4.5386e-01  1.2555e+00 -6.6705e-01 -3.0835e-01 -1.4692e-01  3.9952e+00\n",
      " -1.1289e+00  1.7926e-01 -5.8095e-01  6.2500e-01  2.6151e+00 -1.3212e+00\n",
      " -1.9355e+00  2.4898e+00 -1.7301e+00  8.6154e-01 -2.5272e+00 -3.0166e+00\n",
      "  5.8867e-01 -1.4207e-01  1.7611e+00 -1.0399e+00  5.7063e-01  7.2554e-01\n",
      "  8.7684e-01  1.6996e+00  1.1084e+00 -2.6877e-01 -2.2970e+00  1.8850e+00\n",
      "  1.2536e-01  1.1671e+00 -1.3038e+00  8.2640e-01  3.8002e-01 -1.0354e+00\n",
      "  2.2547e+00 -2.4526e-01  8.3622e-01 -7.9525e-01  1.4251e+00  4.5795e-02\n",
      "  2.5062e+00  7.2175e-01 -3.8197e-01  1.1387e+00  1.6779e+00 -1.6925e+00\n",
      "  8.2290e-02 -3.3117e-01 -6.1974e-01  8.0306e-01  2.1189e+00 -7.8231e-01\n",
      "  3.3266e-01 -7.1646e-01  1.7721e+00 -1.2573e+00  1.5601e-01  9.6129e-01\n",
      "  4.8986e-01  3.1657e+00 -3.0332e+00 -9.1339e-01 -2.8627e-01 -9.8174e-01\n",
      " -2.7599e+00  2.9160e+00 -1.1540e+00  2.9537e+00 -1.1502e+00 -2.2760e+00\n",
      "  1.1679e+00  2.0314e-01 -2.2882e+00  1.0485e+00 -4.9545e-01  2.4065e+00\n",
      "  9.1433e-01  1.3808e-01 -1.1361e+00 -1.2278e+00 -1.8958e+00 -2.4934e-01\n",
      " -4.1792e+00 -1.3781e+00 -8.9829e-01 -1.4802e+00 -1.2894e+00  2.2336e+00\n",
      " -2.4411e+00  3.0410e-01  2.1452e+00  1.8540e+00 -3.3640e-01  4.2768e-01\n",
      " -4.2577e-01  2.2337e-01  3.3650e+00 -2.1996e-01 -1.7426e+00  1.4554e+00\n",
      "  2.5087e+00 -1.1257e+00  1.4096e+00  1.3704e-01  7.6963e-01 -7.6688e-02\n",
      " -6.1875e-01 -2.0445e-01  4.9965e-01 -6.7246e-01 -4.0058e-01 -1.1536e+00\n",
      " -9.2837e-01 -2.5444e+00 -1.9880e-01 -2.1432e+00 -1.8218e+00 -5.7831e-01\n",
      " -1.6785e+00  2.6610e+00  1.0343e+00 -1.4297e+00  1.4563e+00 -2.0388e+00\n",
      " -1.4377e+00 -9.7985e-01  2.9488e+00 -2.7107e-01 -2.4010e-01 -1.3845e-01]\n",
      "Vector length: 300\n"
     ]
    }
   ],
   "source": [
    "print(bananas_vector_lg)\n",
    "print(\"Vector length:\", len(bananas_vector_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing similarities\n",
    "\n",
    "In this part, you’ll be using spaCy’s similarity methods to compare Doc, Token and Span objects and get similarity scores.\n",
    "\n",
    "***Note.*** *SpaCy uses cosine similarity as default.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573941222194427\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day.\")\n",
    "doc2 = nlp(\"It's sunny outside.\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - \"bananas\" similarity\n",
    "\n",
    "- Use `en_core_web_lg` model\n",
    "- Use the `token.similarity` method to compare `bananas` to words:\n",
    "\n",
    "```python\n",
    "words = \"bananas banana apple apples orange oranges monkey monkeys cat human car space\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 5:\n",
    "## Your code here \n",
    "\n",
    "\n",
    "# Define the list of words to compare\n",
    "words = \"bananas banana apple apples orange oranges monkey monkeys cat human car space\"\n",
    "\n",
    "# Split the words string into a list\n",
    "word_list = words.split()\n",
    "\n",
    "# Calculate the similarity between \"bananas\" and each word in the list\n",
    "bananas_similarities = []  \n",
    "for word in word_list:\n",
    "    doc1 = nlp(\"bananas\")\n",
    "    doc2 = nlp(word)\n",
    "    similarity = doc1.similarity(doc2)  # Calculate similarity\n",
    "    bananas_similarities.append([word, similarity])  # Append to the list\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bananas', 1.0]\n",
      "['banana', 0.8712358421094191]\n",
      "['apple', 0.6073679969526968]\n",
      "['apples', 0.7323371266255725]\n",
      "['orange', 0.4965249349408526]\n",
      "['oranges', 0.6361825133240437]\n",
      "['monkey', 0.38314181399815933]\n",
      "['monkeys', 0.4383020464176]\n",
      "['cat', 0.20118522144345108]\n",
      "['human', 0.08909514704072133]\n",
      "['car', 0.09095619833949749]\n",
      "['space', 0.08698710343572413]\n"
     ]
    }
   ],
   "source": [
    "for iii in range(len(bananas_similarities)) :\n",
    "    print(bananas_similarities[iii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Longer text similarity\n",
    "\n",
    "- Use `en_core_web_lg` model\n",
    "- Create function text_similarity(ref, text) that uses `span.similarity` to compare similarities between `ref vs. text`\n",
    "\n",
    "**Note.** Both texts contains same words, but text2 word order is partly random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 6:\n",
    "## Your code here \n",
    "\n",
    "def text_similarity(ref, text):\n",
    "    \n",
    "  doc_ref = nlp(ref)\n",
    "  doc_text = nlp(text)\n",
    "  return doc_ref.similarity(doc_text)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9683894159504514\n",
      "Similarity: 0.9683894156750373\n"
     ]
    }
   ],
   "source": [
    "ref = \"Alice likes bananas and apples. She often eats fruits, when she comes from school.\"\n",
    "text1 = \"Alice likes bananas and apples. She eats fruits all the time, when she comes from work.\"\n",
    "text2 = \"Alice bananas apples likes and. She eats fruits all the time, when she comes from work.\"\n",
    "\n",
    "print('Similarity:', text_similarity(ref, text1))\n",
    "print('Similarity:', text_similarity(ref, text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Longer text similarity with BLEU\n",
    "\n",
    "Create similarity comparisons function with BLEU. You get \"official\" BLEU function with following import. (You may need to install `sacrebleu` library first.)\n",
    "\n",
    "```python\n",
    "from sacrebleu.metrics import BLEU\n",
    "```\n",
    "https://github.com/mjpost/sacrebleu?tab=readme-ov-file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 7:\n",
    "## Your code here \n",
    "\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "bleu = BLEU()\n",
    "\n",
    "def text_bleu(ref, text):\n",
    "    \n",
    "  results = bleu.corpus_score([text], [[ref]])\n",
    "  return results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: BLEU = 54.02 78.9/61.1/47.1/37.5 (BP = 1.000 ratio = 1.118 hyp_len = 19 ref_len = 17)\n",
      "BLEU: BLEU = 27.60 78.9/33.3/17.6/12.5 (BP = 1.000 ratio = 1.118 hyp_len = 19 ref_len = 17)\n"
     ]
    }
   ],
   "source": [
    "print('BLEU:', text_bleu(ref, text1))\n",
    "print('BLEU:', text_bleu(ref, text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are words that are labeled as words that have no information in text analysis. Therefore these words are often removed before text analysis.\n",
    "\n",
    "#### Exercise - Stop words\n",
    "\n",
    "- Count Finnish stop words and print 5 of them\n",
    "- Count English stop words and print 5 of them\n",
    "\n",
    "**Hint.** Stop words are python sets. So you need to convert them to lists before you can select first 5 of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 8:\n",
    "## Your code here \n",
    "from spacy.lang.fi.stop_words import STOP_WORDS as fi_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "# Count Finnish stop words and print 5 of them\n",
    "len_stopwords_fi = len(fi_stop)\n",
    "five_stopwords_fi = list(fi_stop)[:5]\n",
    "\n",
    "# Count English stop words and print 5 of them\n",
    "len_stopwords_en = len(en_stop)\n",
    "five_stopwords_en = list(en_stop)[:5]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FI: 822\n",
      "['niiksi', 'toinen', 'kun', 'josta', 'kenties']\n",
      "EN: 326\n",
      "['no', 'hereby', 'over', 'whole', 'eleven']\n"
     ]
    }
   ],
   "source": [
    "print(\"FI:\", len_stopwords_fi)\n",
    "print(five_stopwords_fi)\n",
    "print(\"EN:\", len_stopwords_en)\n",
    "print(five_stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Removing stop words\n",
    "\n",
    "- Use `\"en_core_web_lg\"` model \n",
    "- Create function `` that filters stopwords from the document\n",
    "- print the original and filtered text\n",
    "\n",
    "**Hint.** You can find stopwords from document by using `token.is_stop` -tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 9:\n",
    "## Your code here \n",
    "\n",
    "def filter_stop_en(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "alice_sentense = 'Alice like bananas and apples. She often eats fruits, when she come from school.'\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice like bananas and apples. She often eats fruits, when she come from school.\n",
      "Alice like bananas apples . eats fruits , come school .\n"
     ]
    }
   ],
   "source": [
    "alice_sentense = 'Alice like bananas and apples. She often eats fruits, when she come from school.'\n",
    "\n",
    "# Print the text excluding stop words\n",
    "print(alice_sentense)\n",
    "print(filter_stop_en(alice_sentense))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Removing stop words (Finnish)\n",
    "\n",
    "- Repeat same with Finnish pipeline `fi_core_news_lg` and sentense  \n",
    "  ```python\n",
    "  alice_lause = 'Alice pitää banaaneista ja omenoista. Hän syö usein hedelmiä koulusta tullessaan.'\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'fi_core_news_lg' (3.7.0) was trained with spaCy v3.7.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "## Task 10:\n",
    "## Your code here \n",
    "\n",
    "nlp = spacy.load(\"fi_core_news_lg\")\n",
    "\n",
    "alice_lause = 'Alice pitää banaaneista ja omenoista. Hän syö usein hedelmiä koulusta tullessaan.'\n",
    "\n",
    "def filter_stop_fi(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice pitää banaaneista ja omenoista. Hän syö usein hedelmiä koulusta tullessaan.\n",
      "Alice pitää banaaneista omenoista . syö hedelmiä koulusta tullessaan .\n"
     ]
    }
   ],
   "source": [
    "alice_lause = 'Alice pitää banaaneista ja omenoista. Hän syö usein hedelmiä koulusta tullessaan.'\n",
    "\n",
    "# Print the text excluding stop words\n",
    "print(alice_lause)\n",
    "print(filter_stop_fi(alice_lause))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "1. What is `\"word vector\"`? And how many features spaCy `\"word vector\"` has?\n",
    "2. What is \"bag of words\"?\n",
    "3. What is lemma? What about lemmatization?\n",
    "4. Why spaCy does not use stemming?\n",
    "5. Based on what Spacy calculates similaties between sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here...*\n",
    "\n",
    "1) **Sanavektori** on sanan numeerinen esitys, joka ilmaisee sanan merkityksen ja sanan suhteet muihin sanoihin. SpaCyn sanavektoreissa on riippuen mallin koosta piirteitä n. 100 - 300.\n",
    "\n",
    "2) **\"Bag of words\"**: Tämä on tekstin esitystapa, jossa teksti kuvataan sanojen esiintymistiheyden perusteella. Sanat ikään kuin laitetaan \"pussiin\", eikä niiden järjestystä oteta huomioon. Esimerkiksi lause \"kissa istuu matolla\" esitettäisiin sanaluettelona, jossa jokainen sana esiintyy kerran. \"Bag of words\" -mallia käytetään usein tekstiluokittelussa ja tiedonhaussa.\n",
    "\n",
    "3) **Lemma** on sanan perusmuoto, esimerkiksi \"juoksen\" lemmana on \"juosta\". Lemmatisointi on prosessi, jossa sana palautetaan sen lemmaan. Se on tärkeää, koska se yhdistää eri taivutusmuodot yhteen perusmuotoon, mikä helpottaa tekstin analysointia. Esimerkiksi \"juoksin\", \"juoksee\" ja \"juoksemme\" lemmatisoidaan kaikki muotoon \"juosta\".\n",
    "\n",
    "4) **Stemming** on prosessi, jossa sanoista poistetaan pääteosa, jolloin saadaan sanavartalo. Esimerkiksi \"juokseminen\" stemmattuna voisi olla \"juoksem\". SpaCy käyttää lemmatisointia stemmingin sijaan, koska lemmatisointi tuottaa yleensä merkitykseltään oikeampia tuloksia. Stemming voi johtaa epäselviin tai virheellisiin sanavartaloihin, jotka eivät ole todellisia sanoja. SpaCy pyrkii säilyttämään sanan merkityksen analyysin aikana.\n",
    "\n",
    "5) **SpaCy laskee lauseiden samankaltaisuuden käyttämällä sanavektoreita (word vectors)**. Nämä vektorit edustavat sanojen merkityksiä moniulotteisessa avaruudessa. Lauseiden samankaltaisuus lasketaan vertaamalla niiden sanavektorien keskiarvojen välistä kosinietäisyyttä. Mitä pienempi kosinietäisyys on, sitä samankaltaisempia lauseet ovat. SpaCy:n mallit on koulutettu suurilla tekstimassoilla, jolloin ne oppivat sanojen ja lauseiden merkityssuhteet.\n",
    "\n",
    "References\n",
    "\n",
    "Vasiliev, Y. (2020). Natural Language Processing with Python and spaCy: A Practical Introduction. No Starch Press.\n",
    "\n",
    "SpaCy documentation (https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your answers by running following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\t 'bananas_vector_sm' is not correct. Please check your answer.\n",
      "\t 'filter_stop_en' is not correct. Please check your answer.\n",
      "Correct answers 11 / 13.\n"
     ]
    }
   ],
   "source": [
    "# Do not change this code!\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../answers/spacy/')\n",
    "from spacy2_check import *\n",
    "\n",
    "print(\"Results:\")\n",
    "correct = spacy2_check(str_to_hash, hash_to_str, text_similarity, text_bleu, filter_stop_en, filter_stop_fi,\n",
    "          bananas_vector_sm, bananas_vector_lg, bananas_similarities, \n",
    "          len_stopwords_fi, five_stopwords_fi, len_stopwords_en, five_stopwords_en)\n",
    "\n",
    "print(\"Correct answers\", correct, \"/ 13.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice work! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
