{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN networks\n",
    "\n",
    "This notebook is for RNN related reflection questions in KAMK Syväoppiminen 1 -course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print your name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise by: Janne Bragge\n"
     ]
    }
   ],
   "source": [
    "## Your code here \n",
    "print(\"Exercise by: Janne Bragge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "1. What is RNN? What about LSTM or GRU?\n",
    "2. When you should initialize RNN hidden layer? And how you should initialize layer?\n",
    "3. What is attention method?\n",
    "4. What is generative network?\n",
    "5. What are Start-of-Sentence and End-of-Sentence markers?\n",
    "6. What is Bleu score?\n",
    "7. What are transformer networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to handle sequential data, such as text, speech, or time series data. Unlike traditional feedforward neural networks, RNNs have a feedback loop that allows them to maintain a memory of past inputs. This memory enables them to process sequences of data, where the order of the data points is important.\n",
    "\n",
    "One of the key challenges in training RNNs is the vanishing gradient problem. This occurs when the gradients used to update the network's weights become very small as they are propagated back through time. This can make it difficult for the network to learn long-range dependencies in the data.\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of RNN that addresses the vanishing gradient problem. LSTMs use a special type of memory cell that can store information for long periods. This allows them to learn long-range dependencies in the data.\n",
    "\n",
    "Gated Recurrent Units (GRUs) are another type of RNN that addresses the vanishing gradient problem. GRUs are similar to LSTMs, but they have a simpler architecture. This makes them faster to train than LSTMs.\n",
    "\n",
    "Overall, RNNs, LSTMs, and GRUs are powerful tools for processing sequential data. They have been used to achieve state-of-the-art results in a variety of tasks, such as natural language processing, speech recognition, and machine translation. (Pointer, 2019)\n",
    "\n",
    "2.\n",
    "\n",
    "The RNN hidden state must be initialized before processing a sequence. The most common method is to initialize the hidden state with a zero vector. This is a simple and effective method, but in some cases, it may be useful to use other initialization methods, such as random values or pre-trained values.\n",
    "\n",
    "Hidden state initialization is important because it can affect network performance. If the hidden state is initialized incorrectly, the network may learn incorrect dependencies or fail to learn long dependencies. (Pointer, 2019)\n",
    "\n",
    "3.\n",
    "\n",
    "The attention method is a technique that allows a neural network to focus on specific parts of the input. This is especially useful for tasks where the input has long dependencies, such as machine translation.\n",
    "\n",
    "The attention mechanism works by allowing the network to calculate weights for each part of the input. These weights indicate how important each part is for a given output. The network then uses these weights to calculate a weighted average of the input, which serves as the network's output. (Pointer, 2019)\n",
    "\n",
    "4.\n",
    "\n",
    "Generative networks are neural networks designed to generate new data that resembles training data. These networks are widely used in tasks such as image generation, text generation, and music generation.\n",
    "\n",
    "One of the most well-known types of generative networks is Generative Adversarial Networks (GANs). GANs consist of two networks: a generator and a discriminator. The generator produces new data, while the discriminator tries to distinguish the generated data from the real data. These two networks are trained together until the generator is able to produce data that the discriminator cannot distinguish from the real data. (Foster, 2023)\n",
    "\n",
    "5.\n",
    "\n",
    "Start-of-Sentence and End-of-Sentence markers are special characters used to indicate the beginning and end of a sequence. These markers are commonly used in natural language processing, especially in tasks such as machine translation and text generation.\n",
    "\n",
    "The Start-of-Sentence marker (often <SOS>) is added to the beginning of the sequence, while the End-of-Sentence marker (often <EOS>) is added to the end of the sequence. These markers help the network learn where the sequence starts and ends. (Foster, 2023)\n",
    "\n",
    "\n",
    "6.\n",
    "\n",
    "The BLEU score is a metric used to evaluate the quality of machine translations. It compares the machine translation to one or more human-made reference translations and calculates a score based on how many n-grams (groups of consecutive words) the machine translation shares with the reference translations.\n",
    "\n",
    "The BLEU score is a common metric in evaluating machine translations because it is automatic and easy to calculate. However, BLEU scores also have limitations because they do not take into account semantic or syntactic structure. (Geekforgeeks, 20.3.2025)\n",
    "\n",
    "7.\n",
    "\n",
    "Transformer networks are a neural network architecture that has revolutionized natural language processing. Unlike RNNs, which process sequences step by step, transformer networks process the entire sequence simultaneously. This allows them to learn long dependencies more efficiently and quickly.\n",
    "\n",
    "Transformer networks use an attention mechanism that allows the network to focus on specific parts of the input. They also use layered interconnected networks that allow the network to learn complex dependencies.\n",
    "\n",
    "Transformer networks have achieved significant results in many natural language processing tasks, such as machine translation, text generation, and question-answering systems. (Builtin, 20.3.2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Foster, D. (2023). Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play (2. painos). O'Reilly Media, Inc.\n",
    "\n",
    "Pointer, I. (2019). Programming PyTorch for Deep Learning: Creating and Deploying Deep Learning Applications. O'Reilly Media, Inc.\n",
    "\n",
    "NLP – BLEU Score for Evaluating Neural Machine Translation – Python, Geekforgeeks (https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/) \n",
    "\n",
    "Transformer neural networks: a step-by-step breakdown, Builtin (https://builtin.com/artificial-intelligence/transformer-neural-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
