{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SpaCy - part 1\n",
    "\n",
    "Let’s get started and try out spaCy! You’ll be able to try out some of the 55+ available languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print your name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise by: Janne Bragge\n"
     ]
    }
   ],
   "source": [
    "## Your code here \n",
    "print(\"Exercise by: Janne Bragge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SpaCy and load pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Import SpaCy \n",
    "\n",
    "- import spacy\n",
    "- print spacy version\n",
    "- download SpaCy text processing pipeline `en_core_web_sm` if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.4\n"
     ]
    }
   ],
   "source": [
    "## Task 1:\n",
    "## Your code here \n",
    "#3.7.2\n",
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy pipelines\n",
    "### Inspecting the pipeline\n",
    "\n",
    "Let’s inspect the small English model’s pipeline!\n",
    "\n",
    "- Load the `en_core_web_sm` model and create the `nlp` object.\n",
    "- Print the names of the pipeline components using `nlp.pipe_names`.\n",
    "- Print the full pipeline of `(name, component)`\n",
    " tuples using `nlp.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f4c4ded4c50>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f4c4ded4b90>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f4c52a0dee0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f4c4dd0c550>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f4c4dd0b410>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f4c524d9620>)]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - What happens when you call nlp?\n",
    "\n",
    "What does spaCy do when you call nlp on a string of text?\n",
    "\n",
    "```python\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here...\n",
    "\n",
    "\"Tokenize the text and apply each pipeline component in order\"\n",
    "\n",
    "When spaCy's nlp function is called on a string, it performs several natural language processing steps. First, the text is broken down into tokens, which are individual words and punctuation marks. After that, each token is assigned its part of speech, such as noun or verb. The sentence structure is parsed, and named entities, like people or places, are identified. Additionally, numerical vector representations are created for the words, describing their semantic meaning. Finally, a Doc object is created, containing all the processed information. \n",
    "\n",
    "(https://spacy.pythonhumanities.com/01_02_linguistic_annotations.html)\n",
    "(https://gitlab.dclabra.fi/wiki/s/BJsE3w2Hd)\n",
    "(https://course.spacy.io/en/chapter3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Use cases for custom components\n",
    "\n",
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. Updating the pre-trained models and improving their predictions\n",
    "2. Computing your own values based on tokens and their attributes\n",
    "3. Adding named entities, for example based on a dictionary\n",
    "4. Implementing support for an additional language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here...*\n",
    "\n",
    "2. Computing your own values based on tokens and their attribute, \n",
    "3. Adding named entities, for example based on a dictionary\n",
    "\n",
    "(https://course.spacy.io/en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP pipelines, tokens and token attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - NLP pipeline \n",
    "\n",
    "Create function `return_verbs()` that\n",
    "\n",
    "- load SpaCy text processing pipeline `en_core_web_sm` to variable `nlp`\n",
    "- create a `doc` using nlp pipeline\n",
    "- print part of speech (= sanaluokka) for 5 first tokens\n",
    "- return verb list of all verbs in `doc`\n",
    "\n",
    "\n",
    "```python\n",
    "def return_verbs(text) :\n",
    "\n",
    "    # 1. Load the small English model\n",
    "    \n",
    "    # 2. Process the text\n",
    "    \n",
    "    # 3. Print first 5 tokens and part-of-speechs - hint. use token.pos_ tag\n",
    "    \n",
    "    # 4. Find all verbs in doc \n",
    "    \n",
    "    return verb_list\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2:\n",
    "## Your code here \n",
    "\n",
    "def return_verbs(text):\n",
    "    \"\"\"\n",
    "    Lataa pienen englanninkielisen mallin, prosessoi tekstin,\n",
    "    tulostaa viisi ensimmäistä tokenia ja niiden sanaluokat,\n",
    "    ja palauttaa listan verbeistä.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Lataa pieni englanninkielinen malli\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # 2. Prosessoi teksti\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # 3. Tulosta viisi ensimmäistä tokenia ja niiden sanaluokat\n",
    "    print(\"First 5 tokens and part-of-speechs:\")\n",
    "    for token in doc[:5]:\n",
    "        print(f\"{token.text:<15} {token.pos_:<10}\")\n",
    "\n",
    "    # 4. Etsi kaikki verbit doc-objektista\n",
    "    verb_list = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "\n",
    "    return verb_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tokens and part-of-speechs:\n",
      "It              PRON      \n",
      "’s              VERB      \n",
      "official        NOUN      \n",
      ":               PUNCT     \n",
      "Apple           PROPN     \n",
      "\n",
      "Text verbs ['’s', 'reach']\n",
      "Total: 2\n"
     ]
    }
   ],
   "source": [
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "verbs = return_verbs(text)\n",
    "\n",
    "print(\"\\nText verbs\", verbs)\n",
    "print(\"Total:\", len(verbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Token lemma\n",
    "\n",
    "Create function `return_lemmas()` that\n",
    "\n",
    "- load SpaCy text processing pipeline `en_core_web_sm` to variable `nlp`\n",
    "- create a `doc` using nlp pipeline\n",
    "- print lemma for 5 first tokens\n",
    "- return lemma list of all tokens in `doc`\n",
    "\n",
    "\n",
    "```python\n",
    "def return_lemmas(text) :\n",
    "\n",
    "    # 1. Load the small English model and Process the text\n",
    "    \n",
    "    # 2. Print first 5 tokens and lemmas  \n",
    "    \n",
    "    return lemma_list\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 3:\n",
    "## Your code here \n",
    "import spacy\n",
    "\n",
    "def return_lemmas(text):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print(\"First 5 tokens and lemmas:\\n\")\n",
    "\n",
    "    for token in doc[:5]:\n",
    "        print(f\"{token.text:<20} {token.lemma_}\")\n",
    "\n",
    "    lemma_list = [token.lemma_ for token in doc]\n",
    "\n",
    "    return lemma_list\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tokens and lemmas:\n",
      "\n",
      "I                    I\n",
      "was                  be\n",
      "happy                happy\n",
      "when                 when\n",
      "I                    I\n",
      "\n",
      "Text lemmas: ['I', 'be', 'happy', 'when', 'I', 'run', 'with', 'my', 'two', 'old', 'dog', '.']\n",
      "Total: 12\n"
     ]
    }
   ],
   "source": [
    "text = \"I was happy when I ran with my two older dogs.\"\n",
    "lemmas = return_lemmas(text)\n",
    "\n",
    "print(\"\\nText lemmas:\", lemmas)\n",
    "print(\"Total:\", len(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy EntityRecognizer\n",
    "\n",
    "\"A named entity is basically a real-life object which has proper identification and can be denoted with a proper name. Named Entities can be a place, person, organization, time, object, or geographic entity.\n",
    "\n",
    "For example, named entities would be Roger Federer, Honda city, Samsung Galaxy S10. Named entities are usually instances of entity instances. For example, Roger Federer is an instance of a Tennis Player/person, Honda City is an instance of a car and Samsung Galaxy S10 is an instance of a Mobile Phone.\" \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "## Let's see what \"GPE\" means\n",
    "print(spacy.explain(\"GPE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - EntityRecognizer\n",
    "\n",
    "Create function `return_places()` that\n",
    "\n",
    "- load SpaCy text processing pipeline `en_core_web_sm` to variable `nlp`\n",
    "- create a `doc` using nlp pipeline \n",
    "- print first 5 entities in the document\n",
    "- return list of all places in `doc`\n",
    "\n",
    "\n",
    "```python\n",
    "def return_places(text) :\n",
    "\n",
    "    # 1. Load the small English model and process the text\n",
    "    \n",
    "    # 2. Print document entity count\n",
    "    \n",
    "    # 3. Print document entities\n",
    "\n",
    "    # 4. create list of places (entity label == GPE)\n",
    "    \n",
    "    return place_list\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 4:\n",
    "## Your code here \n",
    "\n",
    "import spacy\n",
    "\n",
    "def return_places(text):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print(f\"Document entity count: {len(doc.ents)}\")\n",
    "    print(\"Document entities and entity labels:\\n\")\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text:<20} {ent.label_}\")\n",
    "\n",
    "    place_list = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    \n",
    "    return place_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document entity count: 5\n",
      "Document entities and entity labels:\n",
      "\n",
      "Apple                ORG\n",
      "first                ORDINAL\n",
      "U.S.                 GPE\n",
      "$1 trillion          MONEY\n",
      "Tim Cook             PERSON\n",
      "\n",
      "Text entity places: ['U.S.']\n",
      "Total: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value. Current CEO is Tim Cook.\"\n",
    "places = return_places(text)\n",
    "\n",
    "print(\"\\nText entity places:\", places)\n",
    "print(\"Total:\", len(places))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Add wikipedia search also for persons and organizations:\n",
    "\n",
    "```python\n",
    "def wikipedia_search(text) :\n",
    "\n",
    "    import spacy\n",
    "    from spacy.tokens import Span\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    def get_wikipedia_url(span):\n",
    "        # Get a Wikipedia URL \n",
    "        if span.label_ in (\"GPE\"): ### -1-\n",
    "            entity_text = span.text.replace(\" \", \"_\")\n",
    "            return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "    \n",
    "    \n",
    "    # Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "    Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "    urls = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in (\"GPE\"): ### -2-\n",
    "            print('%-15s' % ent.text, '%-10s' % ent.label_, '%-30s' %ent._.wikipedia_url)\n",
    "            urls.append(ent._.wikipedia_url)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 5:\n",
    "## Your code here \n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in (\"GPE\", \"PERSON\", \"ORG\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "def wikipedia_search(text):\n",
    "    print(\"\\n\")\n",
    "    urls = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in (\"GPE\", \"PERSON\", \"ORG\"):\n",
    "            print('%-15s' % ent.text, '%-10s' % ent.label_, '%-30s' % ent._.wikipedia_url)\n",
    "            urls.append(ent._.wikipedia_url)\n",
    "    return urls\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "David Bowie     PERSON     https://en.wikipedia.org/w/index.php?search=David_Bowie\n",
      "England         GPE        https://en.wikipedia.org/w/index.php?search=England\n",
      "Nokia           ORG        https://en.wikipedia.org/w/index.php?search=Nokia\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/w/index.php?search=David_Bowie',\n",
       " 'https://en.wikipedia.org/w/index.php?search=England',\n",
       " 'https://en.wikipedia.org/w/index.php?search=Nokia']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  \"\"\"\n",
    "        In over fifty years from his very first recordings right through to his \n",
    "        last album, David Bowie was at the vanguard of contemporary culture. He lives in England. \n",
    "        But does he work for Nokia?\n",
    "        \"\"\"\n",
    "\n",
    "urls=wikipedia_search(text)\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Why the code below is bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here\n",
    "\n",
    "The code also assumes that if a proper noun is found, the next token must be a verb. This is an oversimplified assumption that doesn't hold true in many cases.\n",
    "\n",
    "\"Berlin looks like a nice city and Helsinki metropoly attracts many tourists\" -> only Berlin is found.\n",
    "\"Berlin looks like a nice city and Helsinki attracts many tourists\" -> Berlin and Helsinki are found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "1. What is spaCy?\n",
    "2. Can you find Finnish pipelines in SpaCy?\n",
    "3. What’s not included in a model package that you can load into spaCy?\n",
    "    - A meta file including the language, pipeline and license.\n",
    "    - Binary weights to make statistical predictions.\n",
    "    - The labelled data that the model was trained on.\n",
    "    - Strings of the model's vocabulary and their hashes. \n",
    "4. What is `nlp` in these exercises?\n",
    "5. Is it possible to update spaCy models? How?\n",
    "6. Why you should be careful with training data, if you re-train SpaCy model? \n",
    "7. Miksi luonnollisen kielen käsittely on niin vaikeaa?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here...*\n",
    "\n",
    "1. spaCy is a Python library designed for advanced Natural Language Processing. It provides pre-trained models for various languages, container objects representing linguistic elements like sentences and words, and attributes for linguistic features such as parts of speech. spaCy also natively supports advanced features like word vectors and offers visualizers for syntactic structure and named entities. Users can customize models, train new ones, and integrate with other machine learning libraries, making it a versatile tool for NLP tasks. (Vasiliev, 2020)\n",
    "\n",
    "2. Yes, spaCy provides Finnish pipelines.\n",
    "\n",
    "    fi_core_news_sm: A small pipeline, suitable for basic NLP tasks.   \n",
    "    fi_core_news_md: A medium-sized pipeline, offering better accuracy for more complex tasks.\n",
    "    fi_core_news_lg: A large pipeline, providing the highest accuracy but requiring more resources.\n",
    "\n",
    "    (https://spacy.io/models)\n",
    "\n",
    "3. The labelled data that the model was trained on is not included\n",
    "\n",
    "4. It's \"a container of metadata\". In this case, the nlp object is loaded with information from the en_core_web_sm file.\n",
    "\n",
    "5. It's possible to update SpaCy models.  While you can't directly modify the pre-trained models, you can achieve updates through these methods:\n",
    "\n",
    "- Training with new data\n",
    "\n",
    "    This involves adding new labeled data to the existing model and retraining it. This allows the model to  learn new patterns and improve its performance on specific tasks or domains. \n",
    "\n",
    "(https://towardsdatascience.com/improving-the-ner-model-with-patent-texts-spacy-prodigy-and-a-bit-of-magic-44c86282ea99/#:~:text=prodigy%20annotated%20data-,Results,entities%20from%20the%20specific%20domain.)\n",
    "\n",
    "- Custom components\n",
    "\n",
    "    You can create custom pipeline components to add new functionalities or modify existing ones. 1  This allows you to tailor the model to your specific requirements and integrate it with other tools or resources\n",
    "\n",
    "(https://spacy.io/usage/saving-loading)\n",
    "\n",
    "6. Retraining SpaCy models can introduce new challenges. If not done carefully, it can lead to biases in the model, reflecting the biases present in the new training data. This can result in unfair or discriminatory outcomes, especially in sensitive applications like sentiment analysis or text classification.\n",
    "\n",
    "Furthermore, retraining can cause the model to overfit to the new data, performing well on the training data but poorly on unseen data. It can also lead to catastrophic forgetting, where the model forgets previously learned patterns or knowledge, especially if the new data is significantly different from the original training data. (Vasiliev, 2020)\n",
    "\n",
    "(https://spacy.io/usage/training)\n",
    "\n",
    "7. Natural language processing is complex because language is inherently ambiguous. Words can have multiple meanings depending on context, and grammatical structures can be intricate and varied. Informal usage, such as slang, idiomatic expressions, and colloquial constructions, further challenge processing. Language is ever-evolving, with new words and expressions constantly emerging. Moreover, language is multifaceted and encompasses hidden meanings, like sarcasm and jokes, which significantly influence interpretation.\n",
    "\n",
    "vrt. \"kehtaa\" sanan merkitys Pohjanmaalla verrattuna Uudellamaalla. \n",
    "\n",
    "---------------------\n",
    "References \n",
    "\n",
    "Vasiliev, Y. (2020). Natural Language Processing with Python and spaCy: A Practical Introduction. No Starch Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your answers by running following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "First 5 tokens and part-of-speechs:\n",
      "I               PRON      \n",
      "like            VERB      \n",
      "book            NOUN      \n",
      ".               PUNCT     \n",
      "I               PRON      \n",
      "\n",
      "\n",
      "First 5 tokens and lemmas:\n",
      "\n",
      "Running              run\n",
      "is                   be\n",
      "my                   my\n",
      "hoppy                hoppy\n",
      ".                    .\n",
      "\n",
      "\n",
      "Document entity count: 2\n",
      "Document entities and entity labels:\n",
      "\n",
      "Helsinki             GPE\n",
      "Finland              GPE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iivo Niskanen   PERSON     https://en.wikipedia.org/w/index.php?search=Iivo_Niskanen\n",
      "Kuopio          GPE        https://en.wikipedia.org/w/index.php?search=Kuopio\n",
      "Swix            GPE        https://en.wikipedia.org/w/index.php?search=Swix\n",
      "\n",
      "\n",
      "Correct answers 4 / 4.\n"
     ]
    }
   ],
   "source": [
    "# Do not change this code!\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../answers/spacy/')\n",
    "from spacy1_check import *\n",
    "\n",
    "print(\"Results:\")\n",
    "correct = spacy1_check(return_verbs, return_lemmas, return_places, wikipedia_search)\n",
    "\n",
    "print(\"Correct answers\", correct, \"/ 4.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
